const e=JSON.parse('{"key":"v-358928b6","path":"/bigdataComponent/flume/02_Flume%E4%B9%8B%E5%90%84%E7%A7%8DSource%E7%9A%84%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90.html","title":"02_各种Source的介绍及参数解析","lang":"zh-CN","frontmatter":{"title":"02_各种Source的介绍及参数解析","order":1,"author":"xiaovin","date":"2023-04-01T00:00:00.000Z","category":["数据集成"],"sticky":true,"star":true,"description":"一、Source介绍 Source用于对接各种数据源，将收集到的事件发送到临时存储Channel中。 常用的source类型有：Avro Source、Exec Source、Kafka Source、TaildirSource、Spooling Directory Source等，其他类型source请查阅Flume-NG官方介绍。 source类型 说明 1 Thrift Source 支持Thrift协议，Thrift Source监听Thrift端口，接收外部Thrift客户端发送过来的Thrift Event数据。在多级流中，Thrift Source可以和前一个Flume Agent的Thrift Sink配对，建立分层收集拓扑。Thrift Source支持基于Kerberos身份验证的安全模式 2 Kafka Source 从Kafka Topic中读取数据，Kafka Source相当于消息队列的Consumer 3 Exec Source 基于Unix的command在标准输出上生产数据。Exec Source在启动时调用的Unix命令，该命令进程会持续地把标准日志数据输出到Exec Source，如果命令进程关闭，Exec Source也会关闭。Exec Source支持cat [named pipe]或者tail -F [file]命令。Exec Source最大的问题就是数据有可能丢失，因为当Channel接收Exec Source数据出错时或者抛出异常时，Exec Client并不能捕获到该错误 4 Spooling Directory Source 监控指定目录内数据变更。Spooling Directory Source监听系统上的指定目录，当目录中有新的文件创建时，Spooling Directory Source会把新文件的内容读取并推送到Channel中，并且把已读取的文件重命名成指定格式或者把文件删除。由于数据是以文件的形式存放的系统中，Spooling Directory Source的可靠性非常高，即使是Flume Agent崩溃或者重启，数据也可以恢复。Spool Source有2个注意地方，第一个是拷贝到spool目录下的文件不可以再打开编辑，第二个是spool目录下不可包含相应的子目录Spooling Directory Source适合用于同步新文件，但不适合对实时追加日志的文件进行监听并同步。 5 Taildir Source Taildir Source是1.7版本的新特性，综合了Spooling Directory Source和Exec Source的优点。Taildir Source可实时监控一批文件，并记录每个文件最新消费位置，agent进程重启后不会有重复消费的问题。使用时建议用1.8.0版本的flume，1.8.0版本中解决了Taildir Source一个可能会丢数据的bug。 6 Netcat Source 监控某个端口，将流经端口的每一个文本行数据作为Event输入 7 HTTP Source 基于HTTP POST或GET方式的数据源，支持JSON、BLOB表示形式。其中GET主要用户测试，不建议生产环境使用。HTTP数据通过handler（实现HTTPSourceHandler接口）转换成Event，该handler接收HttpServletRequest并返回Event数组。如果handler出现异常，HTTP Source返回400错误。如果Channel满了或者Channel无法接收Event，HTTP Source返回503错误。 8 Syslog Sources 读取syslog数据，产生Event，支持UDP和TCP两种协议。这个Source分成三类SyslogTCP Source、Multiport Syslog TCP Source（多端口）与SyslogUDP Source。其中TCP Source为每一个用回车（\\\\ n）来分隔的字符串创建一个新的事件。而UDP Source将整个消息作为一个单一的事件。 9 JMS Source 从JMS系统（消息、主题）中读取数据，该Source目前只在ActiveMQ中测试 10 Stress Source 压力测试用。StressSource 是内部负载生成source的实现，允许用户配置Event有效载荷的大小。 11 Twitter 1% firehose Source 通过API持续下载Twitter数据，试验性质 12 Scribe Source Scribe是另一种类型的提取系统。采用现有的Scribe提取系统，Flume应该使用基于Thrift的兼容传输协议的ScribeSource。 13 Sequence Generator Source 序列生成器数据源，生产序列数据，实验性质 14 Avro Source 支持Avro协议（Avro RPC），Avro Source监听Avro端口，接收外部Avro客户端发送过来的Avro Event数据。在多级流中，Avro Source可以和前一个Flume Agent的Avro Sink配对，建立分层收集拓扑","head":[["meta",{"property":"og:url","content":"https://xiaoVinCode.github.io/xiaovin/xiaovin/bigdataComponent/flume/02_Flume%E4%B9%8B%E5%90%84%E7%A7%8DSource%E7%9A%84%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90.html"}],["meta",{"property":"og:site_name","content":"文档演示"}],["meta",{"property":"og:title","content":"02_各种Source的介绍及参数解析"}],["meta",{"property":"og:description","content":"一、Source介绍 Source用于对接各种数据源，将收集到的事件发送到临时存储Channel中。 常用的source类型有：Avro Source、Exec Source、Kafka Source、TaildirSource、Spooling Directory Source等，其他类型source请查阅Flume-NG官方介绍。 source类型 说明 1 Thrift Source 支持Thrift协议，Thrift Source监听Thrift端口，接收外部Thrift客户端发送过来的Thrift Event数据。在多级流中，Thrift Source可以和前一个Flume Agent的Thrift Sink配对，建立分层收集拓扑。Thrift Source支持基于Kerberos身份验证的安全模式 2 Kafka Source 从Kafka Topic中读取数据，Kafka Source相当于消息队列的Consumer 3 Exec Source 基于Unix的command在标准输出上生产数据。Exec Source在启动时调用的Unix命令，该命令进程会持续地把标准日志数据输出到Exec Source，如果命令进程关闭，Exec Source也会关闭。Exec Source支持cat [named pipe]或者tail -F [file]命令。Exec Source最大的问题就是数据有可能丢失，因为当Channel接收Exec Source数据出错时或者抛出异常时，Exec Client并不能捕获到该错误 4 Spooling Directory Source 监控指定目录内数据变更。Spooling Directory Source监听系统上的指定目录，当目录中有新的文件创建时，Spooling Directory Source会把新文件的内容读取并推送到Channel中，并且把已读取的文件重命名成指定格式或者把文件删除。由于数据是以文件的形式存放的系统中，Spooling Directory Source的可靠性非常高，即使是Flume Agent崩溃或者重启，数据也可以恢复。Spool Source有2个注意地方，第一个是拷贝到spool目录下的文件不可以再打开编辑，第二个是spool目录下不可包含相应的子目录Spooling Directory Source适合用于同步新文件，但不适合对实时追加日志的文件进行监听并同步。 5 Taildir Source Taildir Source是1.7版本的新特性，综合了Spooling Directory Source和Exec Source的优点。Taildir Source可实时监控一批文件，并记录每个文件最新消费位置，agent进程重启后不会有重复消费的问题。使用时建议用1.8.0版本的flume，1.8.0版本中解决了Taildir Source一个可能会丢数据的bug。 6 Netcat Source 监控某个端口，将流经端口的每一个文本行数据作为Event输入 7 HTTP Source 基于HTTP POST或GET方式的数据源，支持JSON、BLOB表示形式。其中GET主要用户测试，不建议生产环境使用。HTTP数据通过handler（实现HTTPSourceHandler接口）转换成Event，该handler接收HttpServletRequest并返回Event数组。如果handler出现异常，HTTP Source返回400错误。如果Channel满了或者Channel无法接收Event，HTTP Source返回503错误。 8 Syslog Sources 读取syslog数据，产生Event，支持UDP和TCP两种协议。这个Source分成三类SyslogTCP Source、Multiport Syslog TCP Source（多端口）与SyslogUDP Source。其中TCP Source为每一个用回车（\\\\ n）来分隔的字符串创建一个新的事件。而UDP Source将整个消息作为一个单一的事件。 9 JMS Source 从JMS系统（消息、主题）中读取数据，该Source目前只在ActiveMQ中测试 10 Stress Source 压力测试用。StressSource 是内部负载生成source的实现，允许用户配置Event有效载荷的大小。 11 Twitter 1% firehose Source 通过API持续下载Twitter数据，试验性质 12 Scribe Source Scribe是另一种类型的提取系统。采用现有的Scribe提取系统，Flume应该使用基于Thrift的兼容传输协议的ScribeSource。 13 Sequence Generator Source 序列生成器数据源，生产序列数据，实验性质 14 Avro Source 支持Avro协议（Avro RPC），Avro Source监听Avro端口，接收外部Avro客户端发送过来的Avro Event数据。在多级流中，Avro Source可以和前一个Flume Agent的Avro Sink配对，建立分层收集拓扑"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-06-22T07:07:56.000Z"}],["meta",{"property":"article:author","content":"xiaovin"}],["meta",{"property":"article:published_time","content":"2023-04-01T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-06-22T07:07:56.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"02_各种Source的介绍及参数解析\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-04-01T00:00:00.000Z\\",\\"dateModified\\":\\"2023-06-22T07:07:56.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"xiaovin\\"}]}"]]},"headers":[{"level":2,"title":"一、Source介绍","slug":"一、source介绍","link":"#一、source介绍","children":[{"level":3,"title":"1、Avro Source","slug":"_1、avro-source","link":"#_1、avro-source","children":[]},{"level":3,"title":"2、Kafka Source","slug":"_2、kafka-source","link":"#_2、kafka-source","children":[]},{"level":3,"title":"3、Exec Source","slug":"_3、exec-source","link":"#_3、exec-source","children":[]},{"level":3,"title":"4、Spooling Directory Source","slug":"_4、spooling-directory-source","link":"#_4、spooling-directory-source","children":[]},{"level":3,"title":"5、Taildir Source (建议修改源码，后续补充相关操作文章)","slug":"_5、taildir-source-建议修改源码-后续补充相关操作文章","link":"#_5、taildir-source-建议修改源码-后续补充相关操作文章","children":[]}]}],"git":{"createdTime":1687417676000,"updatedTime":1687417676000,"contributors":[{"name":"v_yangjiahao01","email":"v_yangjiahao01@baidu.com","commits":1}]},"readingTime":{"minutes":9.12,"words":2737},"filePathRelative":"bigdataComponent/flume/02_Flume之各种Source的介绍及参数解析.md","localizedDate":"2023年4月1日","excerpt":"<h2> 一、Source介绍</h2>\\n<p>Source用于对接各种数据源，将收集到的事件发送到临时存储Channel中。</p>\\n<p>常用的source类型有：Avro Source、Exec Source、Kafka Source、TaildirSource、Spooling Directory Source等，其他类型source请查阅Flume-NG官方介绍。</p>\\n<table>\\n<thead>\\n<tr>\\n<th></th>\\n<th>source类型</th>\\n<th>说明</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>1</td>\\n<td>Thrift Source</td>\\n<td>支持Thrift协议，Thrift Source监听Thrift端口，接收外部Thrift客户端发送过来的Thrift Event数据。在多级流中，Thrift Source可以和前一个Flume Agent的Thrift Sink配对，建立分层收集拓扑。Thrift Source支持基于Kerberos身份验证的安全模式</td>\\n</tr>\\n<tr>\\n<td>2</td>\\n<td>Kafka Source</td>\\n<td>从Kafka Topic中读取数据，Kafka Source相当于消息队列的Consumer</td>\\n</tr>\\n<tr>\\n<td>3</td>\\n<td>Exec Source</td>\\n<td>基于Unix的command在标准输出上生产数据。Exec Source在启动时调用的Unix命令，该命令进程会持续地把标准日志数据输出到Exec Source，如果命令进程关闭，Exec Source也会关闭。Exec Source支持cat [named pipe]或者tail -F [file]命令。Exec Source最大的问题就是数据有可能丢失，因为当Channel接收Exec Source数据出错时或者抛出异常时，Exec Client并不能捕获到该错误</td>\\n</tr>\\n<tr>\\n<td>4</td>\\n<td>Spooling Directory Source</td>\\n<td>监控指定目录内数据变更。Spooling Directory Source监听系统上的指定目录，当目录中有新的文件创建时，Spooling Directory Source会把新文件的内容读取并推送到Channel中，并且把已读取的文件重命名成指定格式或者把文件删除。由于数据是以文件的形式存放的系统中，Spooling Directory Source的可靠性非常高，即使是Flume Agent崩溃或者重启，数据也可以恢复。Spool Source有2个注意地方，第一个是拷贝到spool目录下的文件不可以再打开编辑，第二个是spool目录下不可包含相应的子目录Spooling Directory Source适合用于同步新文件，但不适合对实时追加日志的文件进行监听并同步。</td>\\n</tr>\\n<tr>\\n<td>5</td>\\n<td>Taildir Source</td>\\n<td>Taildir Source是1.7版本的新特性，综合了Spooling Directory Source和Exec Source的优点。Taildir Source可实时监控一批文件，并记录每个文件最新消费位置，agent进程重启后不会有重复消费的问题。使用时建议用1.8.0版本的flume，1.8.0版本中解决了Taildir Source一个可能会丢数据的bug。</td>\\n</tr>\\n<tr>\\n<td>6</td>\\n<td>Netcat Source</td>\\n<td>监控某个端口，将流经端口的每一个文本行数据作为Event输入</td>\\n</tr>\\n<tr>\\n<td>7</td>\\n<td>HTTP Source</td>\\n<td>基于HTTP POST或GET方式的数据源，支持JSON、BLOB表示形式。其中GET主要用户测试，不建议生产环境使用。HTTP数据通过handler（实现HTTPSourceHandler接口）转换成Event，该handler接收HttpServletRequest并返回Event数组。如果handler出现异常，HTTP Source返回400错误。如果Channel满了或者Channel无法接收Event，HTTP Source返回503错误。</td>\\n</tr>\\n<tr>\\n<td>8</td>\\n<td>Syslog Sources</td>\\n<td>读取syslog数据，产生Event，支持UDP和TCP两种协议。这个Source分成三类SyslogTCP Source、Multiport Syslog TCP Source（多端口）与SyslogUDP Source。其中TCP Source为每一个用回车（\\\\ n）来分隔的字符串创建一个新的事件。而UDP Source将整个消息作为一个单一的事件。</td>\\n</tr>\\n<tr>\\n<td>9</td>\\n<td>JMS Source</td>\\n<td>从JMS系统（消息、主题）中读取数据，该Source目前只在ActiveMQ中测试</td>\\n</tr>\\n<tr>\\n<td>10</td>\\n<td>Stress Source</td>\\n<td>压力测试用。StressSource 是内部负载生成source的实现，允许用户配置Event有效载荷的大小。</td>\\n</tr>\\n<tr>\\n<td>11</td>\\n<td>Twitter 1% firehose Source</td>\\n<td>通过API持续下载Twitter数据，试验性质</td>\\n</tr>\\n<tr>\\n<td>12</td>\\n<td>Scribe Source</td>\\n<td>Scribe是另一种类型的提取系统。采用现有的Scribe提取系统，Flume应该使用基于Thrift的兼容传输协议的ScribeSource。</td>\\n</tr>\\n<tr>\\n<td>13</td>\\n<td>Sequence Generator Source</td>\\n<td>序列生成器数据源，生产序列数据，实验性质</td>\\n</tr>\\n<tr>\\n<td>14</td>\\n<td>Avro Source</td>\\n<td>支持Avro协议（Avro RPC），Avro Source监听Avro端口，接收外部Avro客户端发送过来的Avro Event数据。在多级流中，Avro Source可以和前一个Flume Agent的Avro Sink配对，建立分层收集拓扑</td>\\n</tr>\\n</tbody>\\n</table>","copyright":{"author":"xiaovin"},"autoDesc":true}');export{e as data};
