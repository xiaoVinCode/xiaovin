import{_ as a}from"./plugin-vue_export-helper-c27b6911.js";import{o as n,c as t,f as e}from"./app-06e284b9.js";const s={},d=e(`<h2 id="一、配置详解" tabindex="-1"><a class="header-anchor" href="#一、配置详解" aria-hidden="true">#</a> 一、配置详解</h2><table><thead><tr><th>序号</th><th>参数名</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>1</td><td>type</td><td>Sink类型为hdfs</td><td>-</td></tr><tr><td>2</td><td>hdfs.path</td><td>-</td><td>HDFS存储路径，支持按照时间分区。集群的NameNode名字：单节点：hdfs://主机名(ip):9000/%Y/%m/%d/%H；HA集群：hdfs://nameservice(高可用NameNode服务名称)/%Y/%m/%d/%H</td></tr><tr><td>3</td><td>hdfs.filePrefix</td><td>FlumeData</td><td>Event输出到HDFS的文件名前缀</td></tr><tr><td>4</td><td>hdfs.fileSuffix</td><td>-</td><td>Event输出到HDFS的文件名后缀</td></tr><tr><td>5</td><td>hdfs.inUsePrefix</td><td>-</td><td>临时文件的文件名前缀。Flume首先将Event输出到HDFS指定目录的临时文件中，再根据相关规则重命名为目标文件</td></tr><tr><td>6</td><td>hdfs.inUseSuffix</td><td>.tmp</td><td>临时文件名后缀。</td></tr><tr><td>7</td><td>hdfs.rollInterval</td><td>30</td><td>间隔多久将临时文件滚动成最终目标文件，单位：秒。如果设置为0，则表示不根据时间滚动文件。注：滚动(roll)指的是，HDFS Sink将临时文件重命名成最终目标文件，并新打开一个临时文件来写数据</td></tr><tr><td>8</td><td>hdfs.rollSize</td><td>1024</td><td>当临时文件达到该大小时，滚动成目标文件，单位：byte。该值设置为0，则表示文件不根据文件大小滚动生成</td></tr><tr><td>9</td><td>hdfs.rollCount</td><td>10</td><td>当Event数据达到该数量时，将临时文件滚动生成目标文件。该值设置为0，则表示文件不根据Event数滚动生成</td></tr><tr><td>10</td><td>hdfs.idleTimeout</td><td>0</td><td>当目前被打开的临时文件在该参数指定的时间内，没有任何数据写入，则将该临时文件关闭并重命名成目标文件，单位：秒。该值设置为0，则表示禁用此功能，不自动关闭临时文件</td></tr><tr><td>11</td><td>hdfs.round</td><td>false</td><td>用于HDFS文件按照时间分区，时间戳向下取整</td></tr><tr><td>12</td><td>hdfs.roundValue</td><td>1</td><td>当round设置为true，配合roundUnit时间单位一起使用，例如roundUnit值为minute。该值设置为1则表示一分钟之内的数据写到一个文件中，相当于每一分钟生成一个文件</td></tr><tr><td>13</td><td>hdfs.roundUnit</td><td>second</td><td>按时间分区使用的时间单位，可以选择second（秒）、minute（分钟）、hour（小时）三种粒度的时间单位。示例：a1.sinks.k1.hdfs.path = hdfs://nameservice/flume/events/%y/%m/%d/%H/%M；a1.sinks.k1.hdfs.round = true；a1.sinks.k1.hdfs.roundValue = 10；a1.sinks.k1.hdfs.roundUnit = minute；当时间为2022-04-05 17:38:59时候，hdfs.path依然会被解析为：/flume/events/2022/04/05/17/30；因为设置的是舍弃10分钟内的时间，因此，该目录每10分钟新生成一个</td></tr><tr><td>14</td><td>hdfs.batchSize</td><td>100</td><td>每个批次刷写到HDFS的Event数量</td></tr><tr><td>15</td><td>hdfs.codeC</td><td>不采用压缩</td><td>文件压缩格式，目前支持的压缩格式有gzip、bzip2、lzo、lzop、snappy</td></tr><tr><td>16</td><td>hdfs.fileType</td><td>SequenceFile</td><td>文件类型，包括：SequenceFile、DataStream、CompressedStream。该值设置为DataStream，则输出的文件不会进行压缩，不需要设置hdfs.codeC指定压缩格式。该值设置为CompressedStream，则对输出的文件进行压缩，需要设置hdfs.codeC指定压缩格式</td></tr><tr><td>17</td><td>hdfs.maxOpenFiles</td><td>5000</td><td>最大允许打开的HDFS文件数，当打开的文件数达到该值，则最早打开的文件将会被关闭</td></tr><tr><td>18</td><td>hdfs.minBlockReplicas</td><td>HDFS副本数</td><td>写入HDFS文件块的最小副本数。该参数会影响文件的滚动配置，一般将该参数配置成1，才可以按照配置正确滚动文件</td></tr><tr><td>19</td><td>hdfs.writeFormat</td><td>Writable</td><td>文件的格式，目前可以选择Text或者Writable两种格式</td></tr><tr><td>20</td><td>hdfs.callTimeout</td><td>10000</td><td>操作HDFS文件的超时时间，如果需要写入HDFS文件的Event数比较大或者发生了打开、写入、刷新、关闭文件超时的问题，可以根据实际情况适当增大超时时间，单位：毫秒</td></tr><tr><td>21</td><td>hdfs.threadsPoolSize</td><td>10</td><td>每个HDFS Sink执行HDFS IO操作打开的线程数</td></tr><tr><td>22</td><td>hdfs.rollTimerPoolSize</td><td>1</td><td>HDFS Sink根据时间滚动生成文件时启动的线程数</td></tr><tr><td>23</td><td>hdfs.timeZone</td><td>Local Time本地时间</td><td>写入HDFS文件使用的时区</td></tr><tr><td>24</td><td>hdfs.useLocalTimeStamp</td><td>false</td><td>是否使用本地时间替换Event头信息中的时间戳</td></tr><tr><td>25</td><td>hdfs.closeTries</td><td>0</td><td>在发起关闭尝试后，尝试重命名临时文件的次数。如果设置为1，表示重命名一次失败后不再继续尝试重命名操作，此时待处理的文件将处于打开状态，扩展名为．tmp。如果设置为0，表示尝试重命名操作次数不受限制，直到文件最终被重命名成功。如果close调用失败，文件可能仍然会处于打开状态，但是文件中的数据将保持完整，文件会在Flume重启后关闭</td></tr><tr><td>26</td><td>hdfs.retryInterval</td><td>180 秒</td><td>连续尝试关闭文件的时间间隔。如果设置为0或小于0的数，第一次尝试关闭文件失败后将不会继续尝试关闭文件，文件将保持打开状态或者以“.tmp”扩展名结尾的临时文件。如果设置为0，表示不尝试，相当于于将hdfs.closeTries设置成1</td></tr><tr><td>27</td><td>serializer</td><td>TEXT</td><td>序列化方式，可选值有TEXT、avro_event或者实现EventSerializer.Builder接口的类</td></tr><tr><td>28</td><td>kerberosPrincipal</td><td>-</td><td>HDFS安全认证kerberos配置</td></tr><tr><td>29</td><td>kerberosKeytab</td><td>-</td><td>HDFS安全认证kerberos配置</td></tr><tr><td>30</td><td>proxyUser</td><td>-</td><td>代理用户</td></tr></tbody></table><blockquote><h3 id="round-与-rollinterval-理解有误" tabindex="-1"><a class="header-anchor" href="#round-与-rollinterval-理解有误" aria-hidden="true">#</a> round 与 rollInterval 理解有误</h3><p>round、roundValue、roundUnit是基于路径path去滚动生成文件夹的，针对文件夹而言</p><p>rollInterval、rollSize、rollCount是基于文件的条件限制滚动生成文件的，基于文件而言的</p></blockquote><h2 id="二、简单模板" tabindex="-1"><a class="header-anchor" href="#二、简单模板" aria-hidden="true">#</a> 二、简单模板</h2><div class="language-properties line-numbers-mode" data-ext="properties"><pre class="language-properties"><code><span class="token key attr-name">agent_name.sources</span> <span class="token punctuation">=</span> <span class="token value attr-value">source_name</span>
<span class="token key attr-name">agent_name.channels</span> <span class="token punctuation">=</span> <span class="token value attr-value">channel_name</span>
<span class="token key attr-name">agent_name.sinks</span> <span class="token punctuation">=</span> <span class="token value attr-value">sink_name</span>

<span class="token comment"># source</span>
<span class="token key attr-name">agent_name.sources.source_name.type</span> <span class="token punctuation">=</span> <span class="token value attr-value">avro</span>
XXX
XXX

<span class="token comment"># channel</span>
<span class="token key attr-name">agent_name.channels.channel_name.type</span> <span class="token punctuation">=</span> <span class="token value attr-value">file</span>
XXX
XXX

<span class="token comment"># sink</span>
<span class="token key attr-name">agent_name.sinks.sink_name.type</span> <span class="token punctuation">=</span> <span class="token value attr-value">hdfs</span>
<span class="token key attr-name">agent_name.sinks.sink_name.hdfs.path</span> <span class="token punctuation">=</span> <span class="token value attr-value">hdfs://\${HA_NameNode_Name}/flume_data/yr=%Y/mon=%m/day=%d/hr=%H</span>
<span class="token key attr-name">agent_name.sinks.sink_name.hdfs.writeFormat</span> <span class="token punctuation">=</span> <span class="token value attr-value">Text</span>
<span class="token key attr-name">agent_name.sinks.sink_name.hdfs.fileSuffix</span> <span class="token punctuation">=</span> <span class="token value attr-value">_\${hdfsFileSuffix}.log</span>
<span class="token key attr-name">agent_name.sinks.sink_name.hdfs.fileType</span> <span class="token punctuation">=</span> <span class="token value attr-value">DataStream</span>
<span class="token key attr-name">agent_name.sinks.sink_name.hdfs.filePrefix</span> <span class="token punctuation">=</span> <span class="token value attr-value">%Y%m%d%H%M</span>
<span class="token key attr-name">agent_name.sinks.sink_name.hdfs.useLocalTimeStamp</span> <span class="token punctuation">=</span> <span class="token value attr-value">true</span>
<span class="token key attr-name">agent_name.sinks.sink_name.hdfs.rollInterval</span> <span class="token punctuation">=</span> <span class="token value attr-value">0</span>
<span class="token key attr-name">agent_name.sinks.sink_name.hdfs.rollSize</span> <span class="token punctuation">=</span> <span class="token value attr-value">125829120</span>
<span class="token key attr-name">agent_name.sinks.sink_name.hdfs.rollCount</span> <span class="token punctuation">=</span> <span class="token value attr-value">0</span>
<span class="token key attr-name">agent_name.sinks.sink_name.hdfs.minBlockReplicas</span> <span class="token punctuation">=</span> <span class="token value attr-value">1</span>
<span class="token key attr-name">agent_name.sinks.sink_name.hdfs.round</span> <span class="token punctuation">=</span> <span class="token value attr-value">true</span>
<span class="token key attr-name">agent_name.sinks.sink_name.hdfs.roundValue</span> <span class="token punctuation">=</span> <span class="token value attr-value">1</span>
<span class="token key attr-name">agent_name.sinks.sink_name.hdfs.roundUnit</span> <span class="token punctuation">=</span> <span class="token value attr-value">hour</span>
<span class="token key attr-name">agent_name.sinks.sink_name.hdfs.idleTimeout</span> <span class="token punctuation">=</span> <span class="token value attr-value">600</span>

<span class="token comment"># source | channel | sink 关联</span>
<span class="token key attr-name">agent_name.sources.source_name.channels</span> <span class="token punctuation">=</span> <span class="token value attr-value">channel_name</span>
<span class="token key attr-name">agent_name.sinks.sink_name.channel</span> <span class="token punctuation">=</span> <span class="token value attr-value">channel_name</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="三、注意事项及异常" tabindex="-1"><a class="header-anchor" href="#三、注意事项及异常" aria-hidden="true">#</a> 三、注意事项及异常</h2><ol><li><p>idleTimeout 的设置</p><ul><li><p>设置为0，如果flume程序突然宕机，就会导致 hdfs上的 .tmp后缀的文件无法会更改为完成的文件，造成一种假象，以为该文件正在写入。当程序重启时，就会有两个 .tmp文件。</p></li><li><p>如果idle Timeout有设置值m，当在m秒内没有数据写入，就会把tmp文件改为已完成。后面再有数据过来的时候重新生成.tmp文件。</p></li><li><p>建议：最好设置一个比较大的值，防止小文件产生，若不设置，宕机的话会有tmp文件</p></li><li><p>为了能快速查看到数据，可以设置该值较小，没数据进行就滚动，因为临时文件是不能被Hive查询到，但是这样会产生小文件</p></li></ul></li><li><p>round 与 rollInterval 理解有误</p><ul><li><p>round、roundValue、roundUnit是基于路径path去滚动生成文件夹的，针对文件夹而言</p></li><li><p>rollInterval、rollSize、rollCount是基于文件的条件限制滚动生成文件的，基于文件而言的</p></li></ul></li><li><p>异常：Error while trying to hflushOrSync</p><ul><li>问题排查：通过查看不同Flume的Agent日志发现，同名的文件被不同的Flume Agent打开，在文件第二次打开后，先前打开的Agent拥有的token就失效了，因此无法关闭它，就会不断的报错：Error while trying to hflushOrSync!</li><li>查看之前的flume配置文件发现，每一个Flume-Agent配置的hdfsSink是完全一样的，每个Flume-Agent读取的source相同，有很大概率会出现多个Fume-Agent同时写同名文件，导致部分Flume-Agent无法继续。</li><li>解决方案：不同Flume设置不同的文件后缀名</li></ul></li></ol><h4 id="其它重要点" tabindex="-1"><a class="header-anchor" href="#其它重要点" aria-hidden="true">#</a> 其它重要点：</h4><ol><li><h5 id="flume部署的机器上没有hadoop环境依赖" tabindex="-1"><a class="header-anchor" href="#flume部署的机器上没有hadoop环境依赖" aria-hidden="true">#</a> Flume部署的机器上没有Hadoop环境依赖：</h5><p>为了让Flume能够正常地和HDFS进行交互，需要手动添加Hadoop相关的jar包到Flume的classpath中。常见的Hadoop相关的jar包如下：</p><ul><li>hadoop-common.jar</li><li>hadoop-hdfs.jar</li><li>hadoop-auth.jar</li><li>hadoop-mapreduce-client-core.jar</li><li>commons-configuration.jar</li><li>commons-lang.jar</li><li>commons-collections.jar</li></ul><p>具体来说，如果你使用的是<em>Hadoop</em>2.<em>x</em>版本，那么在HADOOP_HOME目录下应该有以下目录：</p><ul><li>$HADOOP_HOME/share/hadoop/common</li><li>$HADOOP_HOME/share/hadoop/hdfs</li><li>$HADOOP_HOME/share/hadoop/mapreduce</li></ul><p>你可以将上述目录中的对应jar包复制到Flume的lib目录下，即$FLUME_HOME/lib目录下。</p><p>需要注意的是，这些jar包的版本要和你的Hadoop集群的版本保持一致，否则可能会出现不兼容等问题。建议在添加前先确认好版本信息。</p></li><li><h5 id="hadoop的namenode是高可用" tabindex="-1"><a class="header-anchor" href="#hadoop的namenode是高可用" aria-hidden="true">#</a> Hadoop的NameNode是高可用：</h5></li></ol><ul><li><p>将Hadoop集群的core-site.xml和hdfs-site.xml文件复制到Flume服务器上的某个目录（/path/to/hadoop/conf）中</p><ul><li><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>bin/flume-ng agent --conf-file conf/flume.conf <span class="token parameter variable">--name</span> a1 <span class="token parameter variable">-Dhadoop.conf.dir</span><span class="token operator">=</span>/path/to/hadoop/conf
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li></ul></li><li><p>在启动Flume时，也可以使用&quot;-conf&quot;参数指定core-site.xml和hdfs-site.xml文件的路径，如下所示：</p><ul><li><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>bin/flume-ng agent --conf-file conf/flume.conf <span class="token parameter variable">--name</span> a1 <span class="token parameter variable">-conf</span> /path/to/hadoop/conf/core-site.xml <span class="token parameter variable">-conf</span> /path/to/hadoop/conf/hdfs-site.xml
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li></ul></li><li><p>直接两个两个文件放到 Flume_HOME/conf 目录下</p></li></ul><ol start="3"><li><h5 id="没有权限写入hdfs目录" tabindex="-1"><a class="header-anchor" href="#没有权限写入hdfs目录" aria-hidden="true">#</a> 没有权限写入HDFS目录：</h5></li></ol><ul><li><p>可以在启动命令中设置&quot;-Duser.name=kevin&quot;参数</p></li><li><p>需要注意的是，kevin用户需要在HDFS上拥有写入目标目录的权限，否则会出现写入失败等问题。因此，在实际使用中，需要对该用户进行独立管理，并按照实际需求进行授权</p></li></ul><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>./bin/flume-ng agent <span class="token parameter variable">-n</span> <span class="token variable">\${agentName}</span> <span class="token parameter variable">-c</span> <span class="token variable">\${flumeClient}</span>/conf/ <span class="token parameter variable">-f</span> <span class="token variable">\${jobLocation}</span> <span class="token parameter variable">-Duser.name</span><span class="token operator">=</span>kevin
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div>`,13),l=[d];function i(r,o){return n(),t("div",null,l)}const c=a(s,[["render",i],["__file","08_Flume之HDFS-Sink的参数解析及异常处理.html.vue"]]);export{c as default};
