const a=JSON.parse('{"key":"v-ee9fd578","path":"/bigdataComponent/flume/12_Flume%E4%B9%8B%E8%BF%9E%E6%8E%A5ACL%E7%9A%84Kafka.html","title":"12_连接ACL的Kafka","lang":"zh-CN","frontmatter":{"title":"12_连接ACL的Kafka","order":1,"author":"xiaovin","date":"2023-04-01T00:00:00.000Z","category":["数据集成"],"sticky":true,"star":true,"description":"配置例子 # 命名 Agent 上的组件 a_app_info_to_hdfs.sources = s_app_info a_app_info_to_hdfs.channels = c_app_info a_app_info_to_hdfs.sinks = k_app_info ############################################################################# # 数据采集 - Kafka To HDFS # # 数据源： # 类型 = KafkaSource # Topic = app_info_full # # channel： # 类型 = file # 记录 = ${FLUME_JOB_CONFIG_PATH}/log_channel_datas/../ app_info_dataDir | app_info_checkpointDir # # 数据出口： # 类型 = HDFSSink # HDFS Path = hdfs://${hadoopClusterName}/data/origin_data/log/app_info_full/yr=%Y/mon=%m/day=%d/hr=%H # Hive TableName = app_info_full # source a_app_info_to_hdfs.sources.s_app_info.type = org.apache.flume.source.kafka.KafkaSource a_app_info_to_hdfs.sources.s_app_info.batchSize = 5000 a_app_info_to_hdfs.sources.s_app_info.batchDurationMillis = 2000 # a_app_info_to_hdfs.sources.s_app_info.kafka.bootstrap.servers = ${kafkaCluster} a_app_info_to_hdfs.sources.s_app_info.kafka.bootstrap.servers = ${kafkaCluster_acl} a_app_info_to_hdfs.sources.s_app_info.kafka.consumer.security.protocol=SASL_PLAINTEXT a_app_info_to_hdfs.sources.s_app_info.kafka.consumer.sasl.mechanism = PLAIN a_app_info_to_hdfs.sources.s_app_info.kafka.consumer.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\\"${kfk_user}\\" password=\\"${kfk_pwd}\\" ; a_app_info_to_hdfs.sources.s_app_info.kafka.topics = app_info_full a_app_info_to_hdfs.sources.s_app_info.kafka.consumer.group.id = bigdata_flume a_app_info_to_hdfs.sources.s_app_info.kafka.setTopicHeader = true a_app_info_to_hdfs.sources.s_app_info.kafka.topicHeader = topic a_app_info_to_hdfs.sources.s_app_info.interceptors = i1 a_app_info_to_hdfs.sources.s_app_info.interceptors.i1.type= xxx.xxx.xxx.flume.TimestampInterceptor$Builder # channel a_app_info_to_hdfs.channels.c_app_info.type = file a_app_info_to_hdfs.channels.c_app_info.dataDirs = ${exec_log_path}/app_info_dataDir a_app_info_to_hdfs.channels.c_app_info.checkpointDir = ${exec_log_path}/app_info_checkpointDir a_app_info_to_hdfs.channels.c_app_info.capacity = 3000000 a_app_info_to_hdfs.channels.c_app_info.transactionCapacity = 20000 a_app_info_to_hdfs.channels.c_app_info.keep-alive = 5 # sink a_app_info_to_hdfs.sinks.k_app_info.type = hdfs a_app_info_to_hdfs.sinks.k_app_info.hdfs.path = hdfs://${hadoopClusterName}/data/origin_data/log/%{topic}/yr=%Y/mon=%m/day=%d/hr=%H a_app_info_to_hdfs.sinks.k_app_info.hdfs.fileSuffix = _${hdfsFileSuffix}.gz a_app_info_to_hdfs.sinks.k_app_info.hdfs.filePrefix = log_%Y%m%d%H%M a_app_info_to_hdfs.sinks.k_app_info.hdfs.rollInterval = 0 a_app_info_to_hdfs.sinks.k_app_info.hdfs.rollSize = 125829120 a_app_info_to_hdfs.sinks.k_app_info.hdfs.rollCount = 0 a_app_info_to_hdfs.sinks.k_app_info.hdfs.minBlockReplicas = 1 a_app_info_to_hdfs.sinks.k_app_info.hdfs.round = true a_app_info_to_hdfs.sinks.k_app_info.hdfs.roundValue = 1 a_app_info_to_hdfs.sinks.k_app_info.hdfs.roundUnit = hour a_app_info_to_hdfs.sinks.k_app_info.hdfs.idleTimeout = 600 a_app_info_to_hdfs.sinks.k_app_info.hdfs.fileType = CompressedStream a_app_info_to_hdfs.sinks.k_app_info.hdfs.codeC = gzip a_app_info_to_hdfs.sinks.k_app_info.hdfs.writeFormat = Text # source | channel | sink 关联 a_app_info_to_hdfs.sources.s_app_info.channels = c_app_info a_app_info_to_hdfs.sinks.k_app_info.channel = c_app_info #############################################################################","head":[["meta",{"property":"og:url","content":"https://xiaoVinCode.github.io/xiaovin/xiaovin/bigdataComponent/flume/12_Flume%E4%B9%8B%E8%BF%9E%E6%8E%A5ACL%E7%9A%84Kafka.html"}],["meta",{"property":"og:site_name","content":"文档演示"}],["meta",{"property":"og:title","content":"12_连接ACL的Kafka"}],["meta",{"property":"og:description","content":"配置例子 # 命名 Agent 上的组件 a_app_info_to_hdfs.sources = s_app_info a_app_info_to_hdfs.channels = c_app_info a_app_info_to_hdfs.sinks = k_app_info ############################################################################# # 数据采集 - Kafka To HDFS # # 数据源： # 类型 = KafkaSource # Topic = app_info_full # # channel： # 类型 = file # 记录 = ${FLUME_JOB_CONFIG_PATH}/log_channel_datas/../ app_info_dataDir | app_info_checkpointDir # # 数据出口： # 类型 = HDFSSink # HDFS Path = hdfs://${hadoopClusterName}/data/origin_data/log/app_info_full/yr=%Y/mon=%m/day=%d/hr=%H # Hive TableName = app_info_full # source a_app_info_to_hdfs.sources.s_app_info.type = org.apache.flume.source.kafka.KafkaSource a_app_info_to_hdfs.sources.s_app_info.batchSize = 5000 a_app_info_to_hdfs.sources.s_app_info.batchDurationMillis = 2000 # a_app_info_to_hdfs.sources.s_app_info.kafka.bootstrap.servers = ${kafkaCluster} a_app_info_to_hdfs.sources.s_app_info.kafka.bootstrap.servers = ${kafkaCluster_acl} a_app_info_to_hdfs.sources.s_app_info.kafka.consumer.security.protocol=SASL_PLAINTEXT a_app_info_to_hdfs.sources.s_app_info.kafka.consumer.sasl.mechanism = PLAIN a_app_info_to_hdfs.sources.s_app_info.kafka.consumer.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\\"${kfk_user}\\" password=\\"${kfk_pwd}\\" ; a_app_info_to_hdfs.sources.s_app_info.kafka.topics = app_info_full a_app_info_to_hdfs.sources.s_app_info.kafka.consumer.group.id = bigdata_flume a_app_info_to_hdfs.sources.s_app_info.kafka.setTopicHeader = true a_app_info_to_hdfs.sources.s_app_info.kafka.topicHeader = topic a_app_info_to_hdfs.sources.s_app_info.interceptors = i1 a_app_info_to_hdfs.sources.s_app_info.interceptors.i1.type= xxx.xxx.xxx.flume.TimestampInterceptor$Builder # channel a_app_info_to_hdfs.channels.c_app_info.type = file a_app_info_to_hdfs.channels.c_app_info.dataDirs = ${exec_log_path}/app_info_dataDir a_app_info_to_hdfs.channels.c_app_info.checkpointDir = ${exec_log_path}/app_info_checkpointDir a_app_info_to_hdfs.channels.c_app_info.capacity = 3000000 a_app_info_to_hdfs.channels.c_app_info.transactionCapacity = 20000 a_app_info_to_hdfs.channels.c_app_info.keep-alive = 5 # sink a_app_info_to_hdfs.sinks.k_app_info.type = hdfs a_app_info_to_hdfs.sinks.k_app_info.hdfs.path = hdfs://${hadoopClusterName}/data/origin_data/log/%{topic}/yr=%Y/mon=%m/day=%d/hr=%H a_app_info_to_hdfs.sinks.k_app_info.hdfs.fileSuffix = _${hdfsFileSuffix}.gz a_app_info_to_hdfs.sinks.k_app_info.hdfs.filePrefix = log_%Y%m%d%H%M a_app_info_to_hdfs.sinks.k_app_info.hdfs.rollInterval = 0 a_app_info_to_hdfs.sinks.k_app_info.hdfs.rollSize = 125829120 a_app_info_to_hdfs.sinks.k_app_info.hdfs.rollCount = 0 a_app_info_to_hdfs.sinks.k_app_info.hdfs.minBlockReplicas = 1 a_app_info_to_hdfs.sinks.k_app_info.hdfs.round = true a_app_info_to_hdfs.sinks.k_app_info.hdfs.roundValue = 1 a_app_info_to_hdfs.sinks.k_app_info.hdfs.roundUnit = hour a_app_info_to_hdfs.sinks.k_app_info.hdfs.idleTimeout = 600 a_app_info_to_hdfs.sinks.k_app_info.hdfs.fileType = CompressedStream a_app_info_to_hdfs.sinks.k_app_info.hdfs.codeC = gzip a_app_info_to_hdfs.sinks.k_app_info.hdfs.writeFormat = Text # source | channel | sink 关联 a_app_info_to_hdfs.sources.s_app_info.channels = c_app_info a_app_info_to_hdfs.sinks.k_app_info.channel = c_app_info #############################################################################"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-06-22T07:07:56.000Z"}],["meta",{"property":"article:author","content":"xiaovin"}],["meta",{"property":"article:published_time","content":"2023-04-01T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-06-22T07:07:56.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"12_连接ACL的Kafka\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-04-01T00:00:00.000Z\\",\\"dateModified\\":\\"2023-06-22T07:07:56.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"xiaovin\\"}]}"]]},"headers":[{"level":2,"title":"配置例子","slug":"配置例子","link":"#配置例子","children":[]}],"git":{"createdTime":1687417676000,"updatedTime":1687417676000,"contributors":[{"name":"v_yangjiahao01","email":"v_yangjiahao01@baidu.com","commits":1}]},"readingTime":{"minutes":0.94,"words":283},"filePathRelative":"bigdataComponent/flume/12_Flume之连接ACL的Kafka.md","localizedDate":"2023年4月1日","excerpt":"<h2> 配置例子</h2>\\n<div class=\\"language-properties line-numbers-mode\\" data-ext=\\"properties\\"><pre class=\\"language-properties\\"><code><span class=\\"token comment\\"># 命名 Agent 上的组件</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sources</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">s_app_info</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.channels</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">c_app_info</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">k_app_info</span>\\n<span class=\\"token comment\\">#############################################################################</span>\\n\\n<span class=\\"token comment\\"># 数据采集 - Kafka To HDFS</span>\\n<span class=\\"token comment\\">#</span>\\n<span class=\\"token comment\\"># 数据源：</span>\\n<span class=\\"token comment\\"># 类型 = KafkaSource</span>\\n<span class=\\"token comment\\"># Topic = app_info_full</span>\\n<span class=\\"token comment\\">#</span>\\n<span class=\\"token comment\\"># channel：</span>\\n<span class=\\"token comment\\"># 类型 = file</span>\\n<span class=\\"token comment\\"># 记录 = ${FLUME_JOB_CONFIG_PATH}/log_channel_datas/../ app_info_dataDir | app_info_checkpointDir</span>\\n<span class=\\"token comment\\">#</span>\\n<span class=\\"token comment\\"># 数据出口：</span>\\n<span class=\\"token comment\\"># 类型 = HDFSSink</span>\\n<span class=\\"token comment\\"># HDFS Path = hdfs://${hadoopClusterName}/data/origin_data/log/app_info_full/yr=%Y/mon=%m/day=%d/hr=%H</span>\\n<span class=\\"token comment\\"># Hive TableName = app_info_full</span>\\n<span class=\\"token comment\\"># source</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sources.s_app_info.type</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">org.apache.flume.source.kafka.KafkaSource</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sources.s_app_info.batchSize</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">5000</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sources.s_app_info.batchDurationMillis</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">2000</span>\\n<span class=\\"token comment\\"># a_app_info_to_hdfs.sources.s_app_info.kafka.bootstrap.servers = ${kafkaCluster}</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sources.s_app_info.kafka.bootstrap.servers</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">${kafkaCluster_acl}</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sources.s_app_info.kafka.consumer.security.protocol</span><span class=\\"token punctuation\\">=</span><span class=\\"token value attr-value\\">SASL_PLAINTEXT</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sources.s_app_info.kafka.consumer.sasl.mechanism</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">PLAIN</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sources.s_app_info.kafka.consumer.sasl.jaas.config</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">org.apache.kafka.common.security.plain.PlainLoginModule required username=\\"${kfk_user}\\" password=\\"${kfk_pwd}\\" ;</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sources.s_app_info.kafka.topics</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">app_info_full</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sources.s_app_info.kafka.consumer.group.id</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">bigdata_flume</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sources.s_app_info.kafka.setTopicHeader</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">true</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sources.s_app_info.kafka.topicHeader</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">topic</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sources.s_app_info.interceptors</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">i1</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sources.s_app_info.interceptors.i1.type</span><span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">xxx.xxx.xxx.flume.TimestampInterceptor$Builder</span>\\n\\n<span class=\\"token comment\\"># channel</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.channels.c_app_info.type</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">file</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.channels.c_app_info.dataDirs</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">${exec_log_path}/app_info_dataDir</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.channels.c_app_info.checkpointDir</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">${exec_log_path}/app_info_checkpointDir</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.channels.c_app_info.capacity</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">3000000</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.channels.c_app_info.transactionCapacity</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">20000</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.channels.c_app_info.keep-alive</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">5</span>\\n\\n<span class=\\"token comment\\"># sink</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.type</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">hdfs</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.hdfs.path</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">hdfs://${hadoopClusterName}/data/origin_data/log/%{topic}/yr=%Y/mon=%m/day=%d/hr=%H</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.hdfs.fileSuffix</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">_${hdfsFileSuffix}.gz</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.hdfs.filePrefix</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">log_%Y%m%d%H%M</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.hdfs.rollInterval</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">0</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.hdfs.rollSize</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">125829120</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.hdfs.rollCount</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">0</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.hdfs.minBlockReplicas</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">1</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.hdfs.round</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">true</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.hdfs.roundValue</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">1</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.hdfs.roundUnit</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">hour</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.hdfs.idleTimeout</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">600</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.hdfs.fileType</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">CompressedStream</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.hdfs.codeC</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">gzip</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.hdfs.writeFormat</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">Text</span>\\n\\n<span class=\\"token comment\\"># source | channel | sink 关联</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sources.s_app_info.channels</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">c_app_info</span>\\n<span class=\\"token key attr-name\\">a_app_info_to_hdfs.sinks.k_app_info.channel</span> <span class=\\"token punctuation\\">=</span> <span class=\\"token value attr-value\\">c_app_info</span>\\n<span class=\\"token comment\\">#############################################################################</span>\\n</code></pre><div class=\\"line-numbers\\" aria-hidden=\\"true\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div></div>","copyright":{"author":"xiaovin"},"autoDesc":true}');export{a as data};
