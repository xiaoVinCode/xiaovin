import{_ as l}from"./plugin-vue_export-helper-c27b6911.js";import{r as o,o as r,c as u,a,b as e,d as s,f as n}from"./app-242a2990.js";const c={},i=n('<h4 id="一、介绍" tabindex="-1"><a class="header-anchor" href="#一、介绍" aria-hidden="true">#</a> 一、介绍</h4><p>Flume是一个开源的分布式日志收集系统，而Kafka是一个高吞吐量的分布式消息系统。</p><p>KafkaSource是Flume中的Source类型之一，可以实现数据从Kafka到Flume的无缝传输。</p><h4 id="二、kafkasource的特性" tabindex="-1"><a class="header-anchor" href="#二、kafkasource的特性" aria-hidden="true">#</a> 二、KafkaSource的特性：</h4><ul><li>可以通过配置选取特定的topic或者全部topic，并可以选择指定partition或全部partition。</li><li>可以支持多线程从Kafka中读取数据并发往Channel中。</li><li>可以配置是否自动提交offset，即保存消费位置的参数，来支持对数据消费的状态做精细的控制。</li><li>可以支持使用Kafka提供的Consumer Group机制，以及基于zookeeper的group机制，处理消息的负载均衡和failover机制。</li><li>可以自定义序列化和反序列化方式，可以配合不同的业务场景使用不同的序列化方案。</li><li>可以实现Kafka和间数据的格式转换。</li></ul><h4 id="三、kafkasource的参数解析和用法" tabindex="-1"><a class="header-anchor" href="#三、kafkasource的参数解析和用法" aria-hidden="true">#</a> 三、KafkaSource的参数解析和用法：</h4>',6),d=a("thead",null,[a("tr",null,[a("th",null,"序号"),a("th",null,"参数"),a("th",null,"默认值"),a("th",null,"描述")])],-1),p=a("tr",null,[a("td",null,"1"),a("td",null,"type"),a("td"),a("td",null,[e("表示该Source的类型，需设置为"),a("code",null,"org.apache.flume.source.kafka.KafkaSource")])],-1),k=a("tr",null,[a("td",null,"2"),a("td",null,"kafka.bootstrap.servers"),a("td",null,"无默认值"),a("td",null,"必填项，表示Kafka服务端的地址。")],-1),m=a("tr",null,[a("td",null,"3"),a("td",null,"kafka.topics"),a("td",null,"无默认值"),a("td",null,"必填项，表示需要消费的topic名称列表，多个用“,”分隔。")],-1),f=a("td",null,"4",-1),v={href:"http://kafka.consumer.group.id/",target:"_blank",rel:"noopener noreferrer"},h=a("td",null,"空字符串",-1),_=a("td",null,"可选项，用于支持Kafka的消费者组机制，保证同一个组内的所有消费者共享partition。",-1),b=a("tr",null,[a("td",null,"5"),a("td",null,"kafka.consumer.auto.offset.reset"),a("td",null,"latest"),a("td",null,[e("可选项，用于设置当消费者刚启动时，处理哪些offset。有三种取值："),a("code",null,"smallest"),e("表示从最小的offset开始消费，"),a("code",null,"largest"),e("表示从最大的offset开始消费，"),a("code",null,"none"),e("表示在没有发现offset时抛出异常。")])],-1),g=a("tr",null,[a("td",null,"6"),a("td",null,"batchSize"),a("td",null,"100"),a("td",null,"该参数控制每次从Kafka获取的消息数量。默认情况下，该参数为100，即每次批量处理100条消息。如果需要一次获取更多的消息，则可以增加该值。但是请注意，过大的batchSize可能会导致一次性获取大量消息并在内存中缓存，从而导致系统性能下降。")],-1),y=a("tr",null,[a("td",null,"7"),a("td",null,"batchDurationMillis"),a("td",null,"1000"),a("td",null,"该参数指定KafkaSource等待消息的时间间隔，以ms为单位。默认情况下，该参数值为1000，即每隔1s检查一次是否有新消息。如果需要更快地获取消息，则可以减小该值。请注意，过于频繁的检查新消息可能会增加网络和CPU负载，进而影响系统性能。")],-1),S=a("tr",null,[a("td",null,"8"),a("td",null,"kafka.consumer.auto.commit.enable"),a("td",null,"true"),a("td",null,"可选项，若设置为true，表示是否自动提交offset；若设置为false，则需要通过Channel Processor手动提交offset，默认true。")],-1),K=a("tr",null,[a("td",null,"9"),a("td",null,"kafka.consumer.max.poll.records"),a("td",null,"500"),a("td",null,"可选项，表示一次最多从Kafka中读取的记录数。")],-1),x=a("tr",null,[a("td",null,"10"),a("td",null,"kafka.key.deserializer"),a("td",null,"org.apache.kafka.common.serialization.StringDeserializer"),a("td",null,"可选项，表示用于反序列化key的Deserializer类，")],-1),L=a("tr",null,[a("td",null,"11"),a("td",null,"kafka.value.deserializer"),a("td",null,"org.apache.kafka.common.serialization.ByteArrayDeserializer。"),a("td",null,"可选项，表示用于反序列化value的Deserializer类")],-1),F=a("tr",null,[a("td",null,"12"),a("td",null,"parseAsFlumeEvent"),a("td",null,"false"),a("td",null,"可选项，表示是否解析成Flume事件，默认为false，即将读取到的数据直接封装为KafkaEvent对象。")],-1),T=a("tr",null,[a("td",null,"13"),a("td",null,"selector.type"),a("td",null,"replicating"),a("td",null,[e("事件选择器类型，可选参数，可选值为 和 "),a("code",null,"multiplexing"),e("。默认值为 "),a("code",null,"replicating"),e("，表示将事件复制到所有连接的 Channel；如果设置为 "),a("code",null,"multiplexing"),e("，则将事件发送到通过拦截器链指定的单个 Channel。")])],-1),z=a("tr",null,[a("td",null,"14"),a("td",null,"selector.optional"),a("td",null,"false"),a("td",null,[e("当上述 "),a("code",null,"selector.type"),e(" 为 "),a("code",null,"multiplexing"),e(" 时，指示是否允许 Channel 缺失，可选参数，默认为 "),a("code",null,"false"),e("。")])],-1),C=a("tr",null,[a("td",null,"15"),a("td",null,"maxConcurrentPartitions"),a("td",null,"1"),a("td",null,"最大并发分区数，可选参数，默认值为 1。该参数指定从多个分区中读取消息的并发度，可以设置为较高的值以提高吞吐量。")],-1),q=a("tr",null,[a("td",null,"16"),a("td",null,"pollTimeout"),a("td",null,"5000"),a("td",null,"从 Kafka 中读取消息的轮询超时时间，另一个可选参数，单位为毫秒，默认值为 5000 毫秒。")],-1),A=a("td",null,"17",-1),w={href:"http://consumer.timeout.ms/",target:"_blank",rel:"noopener noreferrer"},N=a("td",null,"120000",-1),P=a("td",null,"Kafka 消费者客户端等待 Broker 返回消息的响应超时时间，也是一个可选参数，默认值为 毫秒（即 2 分钟）。",-1),E=a("tr",null,[a("td",null,"18"),a("td",null,"kafka.topic.whitelist"),a("td"),a("td",null,"用于白名单过滤，指定需要被消费的topic列表。")],-1),B=a("tr",null,[a("td",null,"19"),a("td",null,"kafka.topic.blacklist"),a("td"),a("td",null,"用于黑名单过滤，指定不需要被消费的topic列表。")],-1),D=a("tr",null,[a("td",null,"20"),a("td",null,"topicHeader"),a("td"),a("td",null,"将消息主题添加到 Flume 事件的头中，可选参数。")],-1),I=a("tr",null,[a("td",null,"21"),a("td",null,"keyHeader"),a("td"),a("td",null,"将消息键添加到 Flume 事件的头中，可选参数。")],-1),X=a("blockquote",null,[a("p",null,[e("除此之外，KafkaSource还有其他参数，例如"),a("code",null,"kafka.consumer.*"),e("系列参数，用于配置Kafka消费者相关参数，"),a("code",null,"kafka.topic.*"),e("系列参数，用于配置Kafka topic参数，以及"),a("code",null,"deserializer.*"),e("系列参数，用于配置数据序列化和反序列化方式等。这些参数的具体含义和用法可以通过查看Flume官方文档或Kafka官方文档进行了解。")])],-1),M={href:"https://flume.liyifeng.org/#kafka-source",target:"_blank",rel:"noopener noreferrer"},H=n(`<figure><img src="https://static-resource-yang.oss-cn-shenzhen.aliyuncs.com/typora_pic/202304091758112.png" alt="image-20230409175830057" tabindex="0" loading="lazy"><figcaption>image-20230409175830057</figcaption></figure><h4 id="四、其它参数" tabindex="-1"><a class="header-anchor" href="#四、其它参数" aria-hidden="true">#</a> 四、其它参数：</h4><table><thead><tr><th>序号</th><th>参数名称</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>1</td><td>kafka.consumer.props</td><td></td><td>Kafka 消费者客户端属性值对，可选参数。该参数指定 Kafka 消费者客户端的属性值对，例如 auto.offset.reset=earliest, enable.auto.commit=false。</td></tr><tr><td>2</td><td>consumer.max.poll.records</td><td>500</td><td>单次读取的最大消息数，可选参数，默认为 500。该参数指定 Kafka 消费者在一次轮询中最多读取的消息数量，建议不要将其设置过大。</td></tr><tr><td>3</td><td>kafka.consumer.security.protocol</td><td>PLAINTEXT</td><td>Kafka 安全协议类型，可选参数，默认值为 PLAINTEXT。其他可选值包括 SASL_PLAINTEXT、SASL_SSL 等。</td></tr><tr><td>4</td><td>kafka.ssl.truststore.location</td><td></td><td>SSL 客户端的信任库位置，可选参数。</td></tr><tr><td>5</td><td>kafka.ssl.truststore.password</td><td></td><td>SSL 客户端的信任库密码，可选参数。</td></tr><tr><td>6</td><td>kafka.ssl.keystore.location</td><td></td><td>SSL 客户端的密钥库位置，可选参数。</td></tr><tr><td>7</td><td>kafka.ssl.keystore.password</td><td></td><td>SSL 客户端的密钥库密码，可选参数。</td></tr><tr><td>8</td><td>kafka.ssl.key.password</td><td></td><td>SSL 密钥库中密钥的密码，可选参数。</td></tr><tr><td>9</td><td>kafka.consumer.headerFilterPattern</td><td></td><td>需要保留的头信息的正则表达式，可选参数。如果设置了该参数，仅保留符合该正则表达式的头信息，不符合的头信息将被删除。</td></tr><tr><td>10</td><td>kafka.consumer.headersToLowerCase</td><td>false</td><td>是否将头信息转换为小写字母，可选参数，默认为 false。如果设置为 true，则将所有头信息转换为小写字母。</td></tr><tr><td>11</td><td>kafka.consumer.ssl.enabled.protocols</td><td>TLSv1.2, TLSv1.1, TLSv1</td><td>SSL 支持的协议集合，用逗号分隔，可选参数，默认值为 TLSv1.2, TLSv1.1, TLSv1。</td></tr><tr><td>12</td><td>kafka.consumer.ssl.truststore.type</td><td>JKS</td><td>Kafka Consumer SSL 客户端的信任库类型，可选参数，默认值为 JKS。</td></tr><tr><td>13</td><td>kafka.consumer.ssl.keystore.type</td><td>JKS</td><td>Kafka Consumer SSL 客户端的密钥库类型，可选参数，默认值为 JKS。</td></tr><tr><td>14</td><td>kafka.consumer.ssl.truststore.algorithm</td><td>SunX509</td><td>Kafka Consumer SSL 客户端的信任库算法，可选参数，默认值为 SunX509。</td></tr><tr><td>15</td><td>kafka.consumer.ssl.keystore.algorithm</td><td>SunX509</td><td>Kafka Consumer SSL 客户端的密钥库算法，可选参数，默认值为 SunX509。</td></tr></tbody></table><h4 id="五、配置示例" tabindex="-1"><a class="header-anchor" href="#五、配置示例" aria-hidden="true">#</a> 五、配置示例</h4><div class="language-properties line-numbers-mode" data-ext="properties"><pre class="language-properties"><code><span class="token comment"># Name the source</span>
<span class="token key attr-name">agent1.sources.kafka-source.type</span> <span class="token punctuation">=</span> <span class="token value attr-value">org.apache.flume.source.kafka.KafkaSource</span>

<span class="token comment"># Set Kafka source properties</span>
<span class="token key attr-name">agent1.sources.kafka-source.kafka.bootstrap.servers</span> <span class="token punctuation">=</span> <span class="token value attr-value">broker1:port,broker2:port,broker3:port</span>
<span class="token key attr-name">agent1.sources.kafka-source.kafka.topics</span> <span class="token punctuation">=</span> <span class="token value attr-value">topic1,topic2,topic3</span>
<span class="token key attr-name">agent1.sources.kafka-source.batchSize</span> <span class="token punctuation">=</span> <span class="token value attr-value">5000</span>
<span class="token key attr-name">agent1.sources.kafka-source.batchDurationMillis</span> <span class="token punctuation">=</span> <span class="token value attr-value">2000</span>
<span class="token key attr-name">agent1.sources.kafka-source.kafka.consumer.group.id</span> <span class="token punctuation">=</span> <span class="token value attr-value">flume_consumer_test</span>
<span class="token key attr-name">agent1.sources.kafka-source.kafka.consumer.auto.commit.enable</span> <span class="token punctuation">=</span> <span class="token value attr-value">true</span>
<span class="token key attr-name">agent1.sources.kafka-source.kafka.consumer.auto.commit.interval.ms</span> <span class="token punctuation">=</span> <span class="token value attr-value">5000</span>
<span class="token key attr-name">agent1.sources.kafka-source.kafka.consumer.auto.offset.reset</span> <span class="token punctuation">=</span> <span class="token value attr-value">earliest</span>
<span class="token key attr-name">agent1.sources.kafka-source.kafka.consumer.max.poll.records</span> <span class="token punctuation">=</span> <span class="token value attr-value">500</span>
<span class="token key attr-name">agent1.sources.kafka-source.kafka.consumer.fetch.max.wait.ms</span> <span class="token punctuation">=</span> <span class="token value attr-value">500</span>
<span class="token key attr-name">agent1.sources.kafka-source.interceptors</span> <span class="token punctuation">=</span> <span class="token value attr-value">i1 i2</span>
<span class="token key attr-name">agent1.sources.kafka-source.interceptors.i1.type</span> <span class="token punctuation">=</span> <span class="token value attr-value">timestamp</span>
<span class="token key attr-name">agent1.sources.kafka-source.interceptors.i2.type</span> <span class="token punctuation">=</span> <span class="token value attr-value">static</span>
<span class="token key attr-name">agent1.sources.kafka-source.interceptors.i2.value</span> <span class="token punctuation">=</span> <span class="token value attr-value">mytag</span>

<span class="token comment"># 进一步调整优化</span>
<span class="token key attr-name">agent1.sources.kafka-source.rebalance.max.retries</span> <span class="token punctuation">=</span> <span class="token value attr-value">10</span>
<span class="token key attr-name">agent1.sources.kafka-source.rebalance.backoff.ms</span> <span class="token punctuation">=</span> <span class="token value attr-value">2000</span>
<span class="token key attr-name">agent1.sources.kafka-source.consumer.timeout.ms</span> <span class="token punctuation">=</span> <span class="token value attr-value">10000</span>
<span class="token key attr-name">agent1.sources.kafka-source.session.timeout.ms</span> <span class="token punctuation">=</span> <span class="token value attr-value">30000</span>
<span class="token key attr-name">agent1.sources.kafka-source.request.timeout.ms</span> <span class="token punctuation">=</span> <span class="token value attr-value">5000</span>
<span class="token key attr-name">agent1.sources.kafka-source.kafka.consumer.fetch.min.bytes</span> <span class="token punctuation">=</span> <span class="token value attr-value">1024</span>
<span class="token key attr-name">agent1.sources.kafka-source.kafka.consumer.fetch.max.bytes</span> <span class="token punctuation">=</span> <span class="token value attr-value">1048576</span>
<span class="token key attr-name">agent1.sources.kafka-source.kafka.consumer.fetch.max.wait.ms</span> <span class="token punctuation">=</span> <span class="token value attr-value">500</span>

<span class="token comment"># Set channel and sink details</span>
<span class="token key attr-name">agent1.sources.kafka-source.channels</span> <span class="token punctuation">=</span> <span class="token value attr-value">channel1</span>
<span class="token key attr-name">agent1.sinks.hdfs-sink.channel</span> <span class="token punctuation">=</span> <span class="token value attr-value">channel1</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><blockquote><p>上述配置中，我们设置了更多的Kafka和Flume参数，包括：</p><ul><li>消费者超时时间<code>consumer.timeout.ms</code>、会话超时时间<code>session.timeout.ms</code>和请求超时时间<code>request.timeout.ms</code>，分别控制消费者在不活动状态下被认为已经超时的时间、消费者与broker失去连接之前的时间、等待broker响应的最长时间。</li><li>自动提交offset<code>auto.commit.enable</code>和自动提交间隔时间<code>auto.commit.interval.ms</code>，用于控制消费者是否自动提交offset以及自动提交的时间间隔。</li><li><code>fetch.max.wait.ms</code>参数指定了poll()方法的最大等待时间，该值越小则会导致消费者的轮询速度变快但也可能会增加网络负载和服务器的负荷。</li><li><code>rebalance.max.retries</code>和<code>rebalance.backoff.ms</code>参数用于控制消费者组rebalance的次数和间隔时间，在发生故障时帮助恢复消费任务。</li><li><code>consumer.fetch.min.bytes</code>、<code>consumer.fetch.max.bytes</code>和<code>consumer.fetch.max.wait.ms</code>参数分别用于控制每次拉取的最小字节数、最大字节数以及最长等待时间。</li></ul></blockquote><h4 id="六、其它要点" tabindex="-1"><a class="header-anchor" href="#六、其它要点" aria-hidden="true">#</a> 六、其它要点</h4><p>A：如果启动多套使用相同的KafkaSource的Flume，是不是保证kafka.consumer.group.id一样，获取的数据就不会出现重复了</p><p>B：是的，如果启动多个使用相同的KafkaSource的Flume，并且它们的<code>kafka.consumer.group.id</code>设置为相同的值，那么它们会组成一个Kafka Consumer Group，其中每个Flume实例只会从分配给其的partition中消费数据，而不会重复消费数据。此时，每个partition只会被一个消费者消费，保证了消费数据的唯一性。</p><p>需要注意的是，如果是多个Flume实例消费同一个topic的数据，即使它们使用不同的<code>kafka.consumer.group.id</code>，也存在数据重复消费的可能性。因为此时不同的Flume实例具有相同的消费权限，可能会订阅到相同的partition并同时消费同样的消息。所以，在这种情况下应该将partition按照一定规则均匀地分配给不同的Flume实例，避免数据重复消费的问题出现。</p><hr><p>A：如果Kafka是5个partition，刚开始只启动一个Flume，是不是5个partition的数据都会发往这个Flume，还是只有一个partition发往Flume？</p><p>B：当只启动一个Flume实例消费一个包含5个partition的Kafka topic时，初始情况下这个Flume实例会分配到这个topic的所有分区，也就是说，5个分区都会发送数据到这个Flume实例中进行消费。</p><p>在Kafka中，每个partition只能被消费者组内的一个消费者消费，如果一个消费者组只有一个消费者，那么该消费者将会消费该topic的所有分区。因此，只有一个Flume实例消费一个包含多个分区的Kafka topic时，它将获取该topic的所有分区的消息，并对其执行相应的处理，避免了数据漏处理的可能性。</p><p>当然，随着Flume实例数量的增加，Kafka将会重新分配分区并将其分配给不同的Flume实例，以达到负载均衡和高可用的目的。</p><h4 id="七、实操" tabindex="-1"><a class="header-anchor" href="#七、实操" aria-hidden="true">#</a> 七、实操</h4><div class="language-properties line-numbers-mode" data-ext="properties"><pre class="language-properties"><code><span class="token key attr-name">agent1.sources.kafka_source.type</span> <span class="token punctuation">=</span> <span class="token value attr-value">org.apache.flume.source.kafka.KafkaSource</span>
<span class="token key attr-name">agent1.sources.kafka_source.batchSize</span> <span class="token punctuation">=</span> <span class="token value attr-value">50000</span>
<span class="token key attr-name">agent1.sources.kafka_source.batchDurationMillis</span> <span class="token punctuation">=</span> <span class="token value attr-value">2000</span>
<span class="token key attr-name">agent1.sources.kafka_source.kafka.bootstrap.servers</span> <span class="token punctuation">=</span> <span class="token value attr-value">\${kafkaCluster_acl}</span>
<span class="token key attr-name">agent1.sources.kafka_source.kafka.consumer.security.protocol</span><span class="token punctuation">=</span><span class="token value attr-value">SASL_PLAINTEXT</span>
<span class="token key attr-name">agent1.sources.kafka_source.kafka.consumer.sasl.mechanism</span> <span class="token punctuation">=</span> <span class="token value attr-value">PLAIN</span>
<span class="token key attr-name">agent1.sources.kafka_source.kafka.consumer.sasl.jaas.config</span> <span class="token punctuation">=</span> <span class="token value attr-value">org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;\${kfk_user}&quot; password=&quot;\${kfk_pwd}&quot; ;</span>
<span class="token key attr-name">agent1.sources.kafka_source.kafka.topics</span> <span class="token punctuation">=</span> <span class="token value attr-value">event_full</span>
<span class="token key attr-name">agent1.sources.kafka_source.kafka.consumer.group.id</span> <span class="token punctuation">=</span> <span class="token value attr-value">bigdata_flume</span>
<span class="token key attr-name">agent1.sources.kafka_source.kafka.setTopicHeader</span> <span class="token punctuation">=</span> <span class="token value attr-value">true</span>
<span class="token key attr-name">agent1.sources.kafka_source.kafka.topicHeader</span> <span class="token punctuation">=</span> <span class="token value attr-value">topic</span>
<span class="token key attr-name">agent1.sources.kafka_source.interceptors</span> <span class="token punctuation">=</span> <span class="token value attr-value">i1</span>
<span class="token key attr-name">agent1.sources.kafka_source.interceptors.i1.type</span><span class="token punctuation">=</span> <span class="token value attr-value">com.yangvin.flume.TimestampInterceptor$Builder</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>3.3 配置优化</p><blockquote><p>主要是在放入flume-channels 的批量数据加大 更改参数: <code>agent1.sources.kafka_source.batchSize = 50000</code><code>agent1.sources.kafka_source.batchDurationMillis = 2000</code> 更改解释:</p><p><strong>即每2秒钟拉取 kafka 一批数据 批数据大小为50000 放入到flume-channels 中 。即flume该节点 flume-channels 输入端数据已放大</strong></p><p>更改依据:</p><ul><li>需要配置kafka单条数据 broker.conf 中配置 <code>message.max.bytes</code></li><li>当前flume channel sink 组件最大消费能力如何?</li></ul></blockquote>`,19);function J(V,$){const t=o("ExternalLinkIcon");return r(),u("div",null,[i,a("table",null,[d,a("tbody",null,[p,k,m,a("tr",null,[f,a("td",null,[a("a",v,[e("kafka.consumer.group.id"),s(t)])]),h,_]),b,g,y,S,K,x,L,F,T,z,C,q,a("tr",null,[A,a("td",null,[a("a",w,[e("consumer.timeout.ms"),s(t)])]),N,P]),E,B,D,I])]),X,a("p",null,[a("a",M,[e("Flume 1.9用户手册中文版"),s(t)])]),H])}const U=l(c,[["render",J],["__file","05_Flume之KafkaSource参数解析.html.vue"]]);export{U as default};
